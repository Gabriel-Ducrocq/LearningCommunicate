{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from bptt import BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PhysicalNet: \n",
    "    \n",
    "    def __init__(self, layer_sizes, input_size, first_input, output_size, keep_prob = 0.9, stddev = 0.001):\n",
    "        self.nb_layers = len(layer_sizes)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.first_input = first_input\n",
    "        self.stddev = stddev\n",
    "        \n",
    "        self.Weights = []\n",
    "        self.Biases = []\n",
    "        self.intermediate_outputs = [self.first_input]\n",
    "        \n",
    "        self.init_weights()\n",
    "        self.init_biases()\n",
    "        self.define_inter_outputs()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for i in range(self.nb_layers):\n",
    "            if i == 0:\n",
    "                W = tf.Variable(tf.random_normal([self.layer_sizes[i], self.input_size], stddev = self.stddev))\n",
    "            elif i != (self.nb_layers - 1):\n",
    "                W = tf.Variable(tf.random_normal([self.layer_sizes[(i+1)], self.layer_sizes[i]], stddev = self.stddev))\n",
    "            else:\n",
    "                W = tf.Variable(tf.random_normal([self.output_size, self.layer_sizes[i]], stddev = self.stddev))\n",
    "                \n",
    "            self.Weights.append(W)\n",
    "            \n",
    "    def init_biases(self):\n",
    "        for i in range(self.nb_layers):\n",
    "            B = tf.Variable(tf.random_normal([self.layer_sizes[i], 1], stddev = self.stddev))\n",
    "            self.Biases.append(B)\n",
    "            \n",
    "    def define_inter_outputs(self): ## ADD THE DROPOUTS !!!\n",
    "        for i in range(self.nb_layers):\n",
    "            W = self.Weights[i]\n",
    "            b = self.Biases[i]\n",
    "            x = self.intermediate_outputs[i]\n",
    "            if i != (self.nb_layers - 1):\n",
    "                o = tf.nn.dropout(tf.nn.elu(tf.matmul(W, x) + b), keep_prob = 0.9)\n",
    "            else:\n",
    "                o = tf.nn.softmax(tf.matmul(W, x) + b)\n",
    "                \n",
    "            self.intermediate_outputs.append(o)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CommunicationNet: ## ADD THE MEMORY !! \n",
    "    \n",
    "    def __init__(self, layer_sizes, input_size, first_input, memory, keep_prob = 0.9, memory_size = 32,\n",
    "                 stddev_epsilon = 0.35, output_size = 256, stddev = 0.001):\n",
    "        self.nb_layers = len(layer_sizes)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.input_size = input_size\n",
    "        self.memory_size = memory_size\n",
    "        self.stddev_epsilon = stddev_epsilon\n",
    "        self.output_size = output_size\n",
    "        self.first_input = first_input\n",
    "        self.stddev = stddev\n",
    "        \n",
    "        self.Weights = []\n",
    "        self.Biases = []\n",
    "        self.Weight_read_mem = tf.Variable(tf.random_normal([self.layer_sizes[(self.nb_layers-1)], self.memory_size]\n",
    "                                                            ,stddev = self.stddev))\n",
    "        self.intermediate_outputs = [first_input]\n",
    "        self.memory = memory\n",
    "        \n",
    "        self.init_weights()    \n",
    "        self.init_biases()\n",
    "        self.define_inter_outputs()\n",
    "        self.def_delta_mem()\n",
    "        \n",
    "        \n",
    "    def init_weights(self):\n",
    "        for i in range(self.nb_layers):\n",
    "            if i == 0:\n",
    "                W = tf.Variable(tf.random_normal([self.layer_sizes[i], self.input_size], stddev = self.stddev))\n",
    "            elif i != (self.nb_layers - 1):\n",
    "                W = tf.Variable(tf.random_normal([self.layer_sizes[(i+1)], self.layer_sizes[i]],stddev = self.stddev))\n",
    "            else:\n",
    "                W = tf.Variable(tf.random_normal([self.output_size, self.layer_sizes[i]],stddev = self.stddev))\n",
    "            self.Weights.append(W)\n",
    "            \n",
    "    def init_biases(self):\n",
    "        for i in range(self.nb_layers):\n",
    "            B = tf.Variable(tf.random_normal([self.layer_sizes[i], 1],stddev = self.stddev))\n",
    "            self.Biases.append(B)\n",
    "            \n",
    "    def define_inter_outputs(self): ## ADD THE DROPOUTS !!!\n",
    "        for i in range(self.nb_layers):\n",
    "            W = self.Weights[i]\n",
    "            b = self.Biases[i]\n",
    "            x = self.intermediate_outputs[i]\n",
    "            if i != (self.nb_layers - 1):\n",
    "                o = tf.nn.dropout(tf.nn.elu(tf.matmul(W, x) + b), keep_prob = 0.9)\n",
    "            else:\n",
    "                o = tf.nn.softmax(tf.matmul(W, x) + tf.matmul(self.Weight_read_mem, self.memory) + b)\n",
    "                \n",
    "            self.intermediate_outputs.append(o)\n",
    "        \n",
    "    def def_delta_mem(self):\n",
    "        self.W_mem = tf.Variable(tf.random_normal(shape =[self.memory_size,self.output_size],stddev = self.stddev))\n",
    "        self.b_mem = tf.Variable(tf.random_normal(shape = [self.memory_size, 1],stddev = self.stddev))\n",
    "        self.output_mem = tf.add(tf.matmul(self.W_mem, self.intermediate_outputs[-1]),self.b_mem)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LastNet: ## ADD THE MEMORY !! The memory initialization is random ==> set it 0\n",
    "    \n",
    "    def __init__(self, layer_sizes, input_size, first_input, memory, keep_prob = 0.9, memory_size = 32, \n",
    "                 stddev_epsilon = 0.35, output_size = 24, stddev = 0.001):\n",
    "        self.nb_layers = len(layer_sizes)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.input_size = input_size\n",
    "        self.memory_size = memory_size\n",
    "        self.stddev_epsilon = stddev_epsilon\n",
    "        self.output_size = output_size\n",
    "        self.first_input = first_input\n",
    "        self.stddev = stddev\n",
    "        \n",
    "        self.Weights = []\n",
    "        self.Biases = []\n",
    "        self.Weight_read_mem = tf.Variable(tf.random_normal([self.output_size, self.memory_size],stddev = self.stddev))\n",
    "        self.intermediate_outputs = [self.first_input]\n",
    "        self.memory = memory\n",
    "        \n",
    "        self.init_weights()    \n",
    "        self.init_biases()\n",
    "        self.define_inter_outputs()\n",
    "        self.def_delta_mem()\n",
    "        \n",
    "        \n",
    "    def init_weights(self):\n",
    "        for i in range(self.nb_layers):\n",
    "            if i == 0:\n",
    "                W = tf.Variable(tf.random_normal([self.layer_sizes[i], self.input_size],stddev = self.stddev))\n",
    "            elif i != (self.nb_layers - 1):\n",
    "                W = tf.Variable(tf.random_normal([self.layer_sizes[(i+1)], self.layer_sizes[i]],stddev = self.stddev))\n",
    "            else:\n",
    "                W = tf.Variable(tf.random_normal([self.output_size, self.layer_sizes[i]],stddev = self.stddev))\n",
    "            self.Weights.append(W)\n",
    "            \n",
    "    def init_biases(self):\n",
    "        for i in range(self.nb_layers):\n",
    "            if i != (self.nb_layers - 1):\n",
    "                B = tf.Variable(tf.random_normal([self.layer_sizes[i+1], 1],stddev = self.stddev))\n",
    "            else:\n",
    "                B = tf.Variable(tf.random_normal([self.output_size, 1], stddev = self.stddev))\n",
    "                \n",
    "            self.Biases.append(B)\n",
    "            \n",
    "    def define_inter_outputs(self): ## ADD THE DROPOUTS !!! REMOVE THE SOFTMAX OF THE LAST LAYER !!!\n",
    "        for i in range(self.nb_layers):\n",
    "            W = self.Weights[i]\n",
    "            b = self.Biases[i]\n",
    "            x = self.intermediate_outputs[i]\n",
    "            if i != (self.nb_layers - 1):\n",
    "                o = tf.nn.dropout(tf.nn.elu(tf.matmul(W, x) + b), keep_prob = 0.9)\n",
    "            else:\n",
    "                o = tf.matmul(W, x) + tf.matmul(self.Weight_read_mem, self.memory) + b\n",
    "                \n",
    "            self.intermediate_outputs.append(o)\n",
    "        \n",
    "    def def_delta_mem(self):\n",
    "        self.W_mem = tf.Variable(tf.random_normal(shape =[self.memory_size,self.output_size],stddev = self.stddev))\n",
    "        self.b_mem = tf.Variable(tf.random_normal(shape = [self.memory_size, 1],stddev = self.stddev))\n",
    "        self.output_mem = tf.add(tf.matmul(self.W_mem, self.intermediate_outputs[-1]),self.b_mem)\n",
    "        \n",
    "    def get_output(self):\n",
    "        return self.intermediate_outputs[-1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## OLD\n",
    "class Policy:\n",
    "    \n",
    "    def __init__(self, nb_agent, nb_landmark, vocabulary_size, hidden_layer_size = 256, env_dim = 2, \n",
    "                 size_goal = 8, memory_size = 32, nb_actions = 3, temperature = 1, batch_size = 1024,\n",
    "                stddev_phys_output = 0.01):\n",
    "        self.stddev_phys_output = stddev_phys_output\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        self.nb_actions = nb_actions\n",
    "        self.memory_size = memory_size\n",
    "        self.agent_name = agent_name\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.env_dim = env_dim\n",
    "        self.nb_agent = nb_agent\n",
    "        self.nb_landmark = nb_landmark\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.size_goal = size_goal\n",
    "        self.goal = tf.placeholder(tf.float32, [self.size_goal, None])\n",
    "        self.memory_last = tf.placeholder(tf.float32, [self.memory_size, None])\n",
    "        self.memorydelta_last = tf.placeholder(tf.float32, [self.memory_size, None])\n",
    "        \n",
    "        self.placeholders_com = []\n",
    "        self.placeholders_phys = []\n",
    "        self.placeholders_mem_comm = []\n",
    "        self.placeholders_deltamem_comm = []\n",
    "        self.networks_com = []\n",
    "        self.networks_phys = []\n",
    "        self.Phi = None\n",
    "        \n",
    "        self.PhiX = None\n",
    "        self.PhiC = None\n",
    "        self.utterances = None\n",
    "        \n",
    "        self.init_all()\n",
    "        \n",
    "    def init_placeholders_com(self):\n",
    "        for i in range(self.nb_agent):\n",
    "            self.placeholders_com.append(tf.placeholder(tf.float32, [self.vocabulary_size, None]))\n",
    "\n",
    "    def init_placeholders_phys(self):\n",
    "        for i in range((self.nb_agent + self.nb_landmark)):\n",
    "            self.placeholders_phys.append(tf.placeholder(tf.float32, [self.env_dim, None]))\n",
    "            \n",
    "    def init_placeholders_mem_comm(self):\n",
    "        for i in range(self.nb_agent):\n",
    "            self.placeholders_mem_comm.append(tf.placeholder(tf.float32, [self.memory_size, None]))\n",
    "    \n",
    "    def init_placeholders_deltamem_comm(self):\n",
    "        for i in range(self.nb_agent):\n",
    "            self.placeholders_deltamem_comm.append(tf.placeholder(tf.float32, [self.memory_size, None]))\n",
    "\n",
    "    def init_com_modules(self):## Les poids seront les mêmes pour tous les agents\n",
    "        with tf.variable_scope(\"communication\") as scope:\n",
    "            self.networks_com.append(CommunicationNet([self.hidden_layer_size, self.hidden_layer_size], \n",
    "                                                 self.vocabulary_size, self.placeholders_com[0], \n",
    "                                                      self.placeholders_mem_comm[0],\n",
    "                                                     self.placeholders_deltamem_comm[0]))\n",
    "        for i in range(1, self.nb_agent):\n",
    "            with tf.variable_scope(\"communication\", reuse=True):\n",
    "                self.networks_com.append(CommunicationNet([self.hidden_layer_size, self.hidden_layer_size], \n",
    "                                                 self.vocabulary_size,self.placeholders_com[i],\n",
    "                                                          self.placeholders_mem_comm[i],\n",
    "                                                         self.placeholders_deltamem_comm[i]))\n",
    "\n",
    "    def init_phys_modules(self):## Les poids seront les mêmes pour tous les agents, il faut rajouter un nom dans le scope\n",
    "        with tf.variable_scope(\"physical\") as scope:\n",
    "            self.networks_phys.append(PhysicalNet([self.hidden_layer_size, self.hidden_layer_size], self.env_dim, \n",
    "                                                  self.placeholders_phys[0],\n",
    "                                                 self.hidden_layer_size))\n",
    "        for i in range(1, (self.nb_agent + self.nb_landmark)):\n",
    "            with tf.variable_scope(\"physical\", reuse = True):\n",
    "                self.networks_phys.append(PhysicalNet([self.hidden_layer_size, self.hidden_layer_size], self.env_dim,\n",
    "                                                      self.placeholders_phys[i],\n",
    "                                                     self.hidden_layer_size))\n",
    "\n",
    "    def init_PhiX(self):\n",
    "        list_outputs = []\n",
    "        for net in self.networks_phys:\n",
    "            list_outputs.append(tf.reshape(net.intermediate_outputs[-1], [256, -1, 1]))\n",
    "\n",
    "        all_phys_output = tf.concat(list_outputs, axis = 2)\n",
    "        self.PhiX = tf.reduce_max(tf.nn.softmax(all_phys_output, dim = -1), axis = 2)\n",
    "\n",
    "    def init_PhiC(self):\n",
    "        list_outputs = []\n",
    "        for net in self.networks_com:\n",
    "            list_outputs.append(tf.reshape(net.intermediate_outputs[-1], [256, -1, 1]))\n",
    "\n",
    "        all_comm_output = tf.concat(list_outputs, axis = 2)\n",
    "        self.PhiC = tf.reduce_max(tf.nn.softmax(all_comm_output, dim = -1), axis = 2) \n",
    "\n",
    "    def init_Phi(self):\n",
    "        self.Phi = tf.concat([self.PhiC, self.goal, self.PhiX], axis = 0)\n",
    "\n",
    "    def init_last_module(self):\n",
    "        inp_size = (2*self.hidden_layer_size + self.size_goal)\n",
    "        self.last_net = LastNet([256, 256], inp_size, self.Phi\n",
    "                                , self.memory_last, self.memorydelta_last)\n",
    "        \n",
    "    def create_feed_dict(self, list_positions, list_utterances, list_mem_comm, list_deltamem_comm, list_mem_last,\n",
    "                  list_detlamem_last, goal):\n",
    "        feed_dict_com = {tensor:com for tensor,com in zip(self.placeholders_com, list_utterances)}\n",
    "        feed_dict_phys = {tensor:phys for tensor,phys in zip(self.placeholders_phys, list_positions)}\n",
    "        feed_dict_mem_com = {tensor:mem for tensor,mem in zip(self.placeholders_mem_comm, list_mem_comm)}\n",
    "        feed_dict_deltamem_com = {tensor:mem for tensor,mem in zip(self.placeholders_deltamem_comm, list_deltamem_comm)}\n",
    "        feed_dict_last = {self.memory_last:list_mem_last[0], self.memorydelta_last:list_deltamem_comm[0]}\n",
    "        feed_dict_goal = {self.goal: goal[0]}\n",
    "        feed_dict_all = {}\n",
    "        feed_dict_all.update(feed_dict_com)\n",
    "        feed_dict_all.update(feed_dict_phys)\n",
    "        feed_dict_all.update(feed_dict_mem_com)\n",
    "        feed_dict_all.update(feed_dict_deltamem_com)\n",
    "        feed_dict_all.update(feed_dict_last)\n",
    "        feed_dict_all.update(feed_dict_goal)\n",
    "        return feed_dict_all\n",
    "        \n",
    "    def init_sample_utterances(self):## Vérifier qu'on prend un bon slice sur l'output\n",
    "        u = -tf.log(-tf.log(tf.random_uniform(shape = [self.vocabulary_size, self.batch_size],dtype=tf.float32)))\n",
    "        utterance_output = tf.slice(self.output, [self.env_dim, 0], [self.vocabulary_size, self.batch_size])\n",
    "        gumbel = tf.exp((utterance_output + u)/self.temperature)\n",
    "        denoms = tf.reduce_sum(gumbel, axis = 0)\n",
    "        self.utterance = gumbel/denoms  \n",
    "        \n",
    "    def init_sample_phys(self):\n",
    "        u = tf.random_normal(shape = [self.env_dim, self.batch_size],dtype=tf.float32, stddev = self.stddev_phys_output)\n",
    "        phys_output = tf.slice(self.output, [0, 0], [self.env_dim, self.batch_size])\n",
    "        self.sample_move = phys_output + u\n",
    "        \n",
    "    def init_output(self):\n",
    "        self.output = self.last_net.get_output()\n",
    "\n",
    "    def init_all(self):\n",
    "        self.init_placeholders_com()\n",
    "        self.init_placeholders_phys()\n",
    "        self.init_placeholders_mem_comm()\n",
    "        self.init_placeholders_deltamem_comm()\n",
    "        self.init_com_modules()\n",
    "        self.init_phys_modules()\n",
    "        self.init_PhiX()\n",
    "        self.init_PhiC()\n",
    "        self.init_Phi()\n",
    "        self.init_last_module()\n",
    "        self.init_output()\n",
    "        self.init_sample_utterances()\n",
    "        self.init_sample_phys()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy_Phys:\n",
    "    \n",
    "    def __init__(self, nb_agent, nb_landmark, list_phys_tensors, hidden_layer_size = 256, env_dim = 2, \n",
    "                 batch_size = 1024, stddev_phys_output = 0.01):\n",
    "        self.stddev_phys_output = stddev_phys_output\n",
    "        self.batch_size = batch_size\n",
    "        self.env_dim = env_dim\n",
    "        self.nb_agent = nb_agent\n",
    "        self.nb_landmark = nb_landmark\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.phys_tensors = list_phys_tensors\n",
    "        self.networks_phys = []\n",
    "        self.PhiX = None\n",
    "        \n",
    "        self.init_all()\n",
    "        \n",
    "    def init_phys_modules(self):\n",
    "        with tf.variable_scope(\"physical\") as scope:\n",
    "            self.networks_phys.append(PhysicalNet([self.hidden_layer_size, self.hidden_layer_size], self.env_dim, \n",
    "                                                  self.phys_tensors[0],\n",
    "                                                 self.hidden_layer_size))\n",
    "        for i in range(1, (self.nb_agent + self.nb_landmark)):\n",
    "            with tf.variable_scope(\"physical\", reuse = True):\n",
    "                self.networks_phys.append(PhysicalNet([self.hidden_layer_size, self.hidden_layer_size], self.env_dim,\n",
    "                                                      self.phys_tensors[i],\n",
    "                                                     self.hidden_layer_size))\n",
    "\n",
    "    def init_PhiX(self):\n",
    "        list_outputs = []\n",
    "        for net in self.networks_phys:\n",
    "            list_outputs.append(tf.reshape(net.intermediate_outputs[-1], [256, -1, 1]))\n",
    "\n",
    "        all_phys_output = tf.concat(list_outputs, axis = 2)\n",
    "        self.PhiX = tf.reduce_max(tf.nn.softmax(all_phys_output, dim = -1), axis = 2)\n",
    "\n",
    "    def init_all(self):\n",
    "        self.init_phys_modules()\n",
    "        self.init_PhiX()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy_Utterance:\n",
    "    \n",
    "    def __init__(self, nb_agent, list_utter_tensors, list_mem_tensors, goal_size, vocabulary_size = 20, \n",
    "                 hidden_layer_size = 256, memory_size = 32, temperature = 1, batch_size = 1024,\n",
    "                 stddev_phys_output = 0.01):\n",
    "        self.size_goal = goal_size\n",
    "        self.nb_agent = nb_agent\n",
    "        self.stddev_phys_output = stddev_phys_output\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        self.memory_size = memory_size\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.goal = tf.placeholder(tf.float32, [self.size_goal, None])\n",
    "        self.memory_last = tf.placeholder(tf.float32, [self.memory_size, None])\n",
    "        \n",
    "        self.com_tensors = list_utter_tensors\n",
    "        self.mem_tensors = list_mem_tensors\n",
    "        self.delta_mem = []\n",
    "        self.networks_com = []\n",
    "\n",
    "        self.PhiC = None\n",
    "        self.init_all()\n",
    "        \n",
    "\n",
    "    def init_com_modules(self):## Les poids seront les mêmes pour tous les agents\n",
    "        with tf.variable_scope(\"communication\") as scope:\n",
    "            self.networks_com.append(CommunicationNet([self.hidden_layer_size, self.hidden_layer_size], \n",
    "                                                 self.vocabulary_size, self.com_tensors[0], \n",
    "                                                      self.mem_tensors[0]))\n",
    "        for i in range(1, self.nb_agent):\n",
    "            with tf.variable_scope(\"communication\", reuse=True):\n",
    "                self.networks_com.append(CommunicationNet([self.hidden_layer_size, self.hidden_layer_size], \n",
    "                                                 self.vocabulary_size,self.com_tensors[i],\n",
    "                                                          self.mem_tensors[i]))\n",
    "\n",
    "    def init_delta_mem_tensors(self):\n",
    "        for i in range(self.nb_agent):\n",
    "            self.delta_mem.append(self.networks_com[i].output_mem)\n",
    "            \n",
    "    def init_PhiC(self):\n",
    "        list_outputs = []\n",
    "        for net in self.networks_com:\n",
    "            list_outputs.append(tf.reshape(net.intermediate_outputs[-1], [256, -1, 1]))\n",
    "\n",
    "        all_comm_output = tf.concat(list_outputs, axis = 2)\n",
    "        self.PhiC = tf.reduce_max(tf.nn.softmax(all_comm_output, dim = -1), axis = 2) \n",
    "\n",
    "    def init_all(self):\n",
    "        self.init_com_modules()\n",
    "        self.init_delta_mem_tensors()\n",
    "        self.init_PhiC()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy_Last:\n",
    "    \n",
    "    def __init__(self, PhiX, PhiC, goal, memory, hidden_layer_size = 256, \n",
    "                 size_goal = 8, memory_size = 32, batch_size = 1024, stddev_phys_output = 0.01, vocabulary_size = 20,\n",
    "                env_dim = 2, temperature = 1):\n",
    "        self.temperature = temperature\n",
    "        self.stddev_phys_output = stddev_phys_output\n",
    "        self.env_dim = env_dim\n",
    "        self.vocabulary_size = vocabulary_size \n",
    "        self.batch_size = batch_size\n",
    "        self.memory_size = memory_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.size_goal = size_goal\n",
    "        self.goal = goal\n",
    "        self.memory_last = memory\n",
    "        self.Phi = None \n",
    "        self.PhiX = PhiX\n",
    "        self.PhiC = PhiC \n",
    "        self.delta_mem = []\n",
    "        self.init_all()\n",
    "\n",
    "    def init_Phi(self):\n",
    "        self.Phi = tf.concat([self.PhiC, self.goal, self.PhiX], axis = 0)\n",
    "\n",
    "    def init_last_module(self):\n",
    "        inp_size = (2*self.hidden_layer_size + self.size_goal)\n",
    "        out_size = self.vocabulary_size + 2*self.env_dim\n",
    "        self.last_net = LastNet([256, 256], inp_size, self.Phi, self.memory_last, output_size = out_size)\n",
    "        \n",
    "    def init_output(self):\n",
    "        self.output = self.last_net.get_output()\n",
    "\n",
    "    def init_all(self):\n",
    "        self.init_Phi()\n",
    "        self.init_last_module()\n",
    "        self.init_output()\n",
    "        self.init_sample_utterances()\n",
    "        self.init_sample_phys()\n",
    "        self.init_delta_mem_tensors()\n",
    "\n",
    "\n",
    "    def init_sample_utterances(self):## Vérifier qu'on prend un bon slice sur l'output\n",
    "        u = -tf.log(-tf.log(tf.random_uniform(shape = [self.vocabulary_size, self.batch_size],dtype=tf.float32)))\n",
    "        utterance_output = tf.slice(self.output, [2*self.env_dim, 0], [self.vocabulary_size, self.batch_size])\n",
    "        gumbel = tf.exp((utterance_output + u)/self.temperature)\n",
    "        denoms = tf.reduce_sum(gumbel, axis = 0)\n",
    "        self.utterance = gumbel/denoms  \n",
    "        \n",
    "    def init_sample_phys(self):\n",
    "        u = tf.random_normal(shape = [2*self.env_dim, self.batch_size],dtype=tf.float32, stddev = self.stddev_phys_output)\n",
    "        self.output = tf.add(tf.slice(self.output, [0, 0], [2*self.env_dim, self.batch_size]), u)\n",
    "        self.sample_move = tf.slice(self.output, [0, 0], [self.env_dim, self.batch_size])\n",
    "        self.sample_gaze  = tf.slice(self.output, [self.env_dim, 0], [self.env_dim, self.batch_size])\n",
    "\n",
    "    def init_delta_mem_tensors(self):\n",
    "        self.delta_mem.append(self.last_net.output_mem)\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy:# Two memories per Agent: one for the communication module, the other one for the last module. Is it correct ?\n",
    "\n",
    "    def __init__(self,nb_agent, nb_landmark, list_phys_tensors, list_utter_tensors, list_mem_tensors, \n",
    "                 list_mem_tensors_last, list_goal_tensors, goal_size, vocabulary_size = 20, hidden_layer_size = 256, \n",
    "                 memory_size = 32, temperature = 1, batch_size = 1024, stddev_phys_output = 0.01, env_dim = 2):\n",
    "        self.nb_agent = nb_agent\n",
    "        self.goal_size = goal_size\n",
    "        self.nb_landmark = nb_landmark\n",
    "        self.list_phys_tensors =  list_phys_tensors\n",
    "        self.list_utter_tensors = list_utter_tensors\n",
    "        self.list_mem_tensors = list_mem_tensors\n",
    "        self.list_goal_tensors = list_goal_tensors\n",
    "        \n",
    "        self.phys_module = Policy_Phys(self.nb_agent, self.nb_landmark, self.list_phys_tensors)\n",
    "        self.utterance_module = Policy_Utterance(self.nb_agent, self.list_utter_tensors, self.list_mem_tensors, \n",
    "                                                 self.goal_size)\n",
    "        \n",
    "        self.list_last_nets = []\n",
    "        self.list_utterance = []\n",
    "        self.list_move = []\n",
    "        self.list_gaze = []\n",
    "        \n",
    "        self.list_delta_mem_comm = self.utterance_module.delta_mem\n",
    "        self.list_delta_mem_last = []\n",
    "        self.list_outputs = []\n",
    "        \n",
    "        self.init_all()\n",
    "        \n",
    "    def init_last_nets(self):### KEEP THE SAME SCOPE IN ORDER TO SHARE THE WEIGHTS !\n",
    "        for i in range(self.nb_agent):\n",
    "            self.list_last_nets.append(Policy_Last(self.phys_module.PhiX, self.utterance_module.PhiC, \n",
    "                                                   self.list_goal_tensors[i], self.list_mem_tensors[i]))\n",
    "            \n",
    "            \n",
    "    def init_output_list(self):\n",
    "        for i in range(self.nb_agent):\n",
    "            self.list_utterance.append(self.list_last_nets[i].utterance)\n",
    "            self.list_move.append(self.list_last_nets[i].sample_move)\n",
    "            self.list_gaze.append(self.list_last_nets[i].sample_gaze)\n",
    "            self.list_delta_mem_last.append(self.list_last_nets[i].delta_mem)\n",
    "            \n",
    "    def init_all(self):\n",
    "        self.init_last_nets()\n",
    "        self.init_output_list()\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, name, name_target, bp, pos, v, gaze, col, vocabulary_size = 20, batch_size = 1024, goals = [0, 0, 1], \n",
    "                 env_dim = 2, goal_size = 8, memory_size = 32, time_delta = 0.1, nb_actions = 3):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.name_target = name_target\n",
    "        self.time_delta = tf.constant([time_delta])\n",
    "        self.env_dim = env_dim\n",
    "        self.memory_size = memory_size\n",
    "        self.name = name\n",
    "        self.goal_size = goal_size\n",
    "        self.batch_size = batch_size\n",
    "        self.bp = bp\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        \n",
    "        self.pos = self.bp.get_past_variable(variable_name = \"pos_\" + self.name, starting_value = \n",
    "                                        tf.placeholder(tf.float32, shape = [self.env_dim, None])) \n",
    "        self.velocity = self.bp.get_past_variable(\"velocity_\" + self.name,tf.placeholder(tf.float32, \n",
    "                                                                                    shape = [self.env_dim, None])) \n",
    "        self.gaze = self.bp.get_past_variable(\"gaze_\" + self.name, tf.placeholder(tf.float32, \n",
    "                                                                             shape = [self.env_dim, None]))\n",
    "        self.goal = tf.placeholder(tf.float32, shape = [self.goal_size, None])\n",
    "        self.utterance = self.bp.get_past_variable(\"utterance_\" + self.name, \n",
    "                                              tf.placeholder(tf.float32, shape = [self.vocabulary_size, None]))\n",
    "        \n",
    "        self.memory = self.bp.get_past_variable(\"memory_\" + self.name, tf.zeros(shape = [self.memory_size, \n",
    "                                                                                         self.batch_size]))\n",
    "        \n",
    "        self.memory_last = self.bp.get_past_variable(\"memory_last_\" + self.name, tf.zeros(shape = [self.memory_size, \n",
    "                                                                                                   self.batch_size]))\n",
    "        \n",
    "        self.tensor_goal_pos = tf.placeholder(tf.float32, shape = [self.env_dim, None])\n",
    "        self.tensor_goal_gaze = tf.placeholder(tf.float32, shape = [self.env_dim, None])\n",
    "        self.tensor_goal_type = tf.placeholder(tf.float32, shape = [self.nb_actions, None])\n",
    "        self.col = col\n",
    "        \n",
    "    def take_step(self, list_positions, list_utterances, list_mem_comm, list_deltamem_comm, list_mem_last,\n",
    "              list_detlamem_last, goal, session):\n",
    "        feed_dict_all = self.create_feed_dict(list_positions, list_utterances, list_mem_comm, list_deltamem_comm, \n",
    "                list_mem_last, list_detlamem_last, goal)\n",
    "        return session.run([self.utterances, self.sample_move], feed_dict = feed_dict_all) \n",
    "\n",
    "    def get_move(self):\n",
    "        return self.p.sample_move\n",
    "    \n",
    "    def get_utterance(self):\n",
    "        return self.p.utterance\n",
    "    \n",
    "    \n",
    "    def compute_reward_agent(self,tensor_agent_pos, tensor_agent_gaze):\n",
    "        ### Il faut prendre la position pour le goal et non pas l'agent spécifique.\n",
    "        r1 = tf.reshape(tf.square(tf.norm(tensor_agent_pos - self.tensor_goal_pos, axis = 0)), [1, self.batch_size])\n",
    "        r2 = tf.reshape(tf.square(tf.norm(tensor_agent_gaze - self.tensor_goal_gaze, axis = 0)), [1, self.batch_size])\n",
    "        utt_norm = tf.square(tf.norm(self.new_utterance, axis = 0))\n",
    "        u_norm = tf.square(tf.norm(tf.concat([self.new_pos, self.new_gaze], axis = 0), axis = 0))\n",
    "        vec = tf.concat([r1, r2, tf.zeros([1,self.batch_size], tf.float32)], axis = 0)\n",
    "        v1 = tf.reduce_sum(tf.multiply(vec, self.tensor_goal_type), axis = 0)\n",
    "        r = -(v1 + utt_norm + u_norm)\n",
    "        return r\n",
    "            \n",
    "    def compute_new_state(self, tensor_utterance, tensor_velocity, tensor_gaze, tensor_memory_delta, \n",
    "                          tensor_memory_last):\n",
    "        ## ADD THE FORCES TO THE NEW VELOCITY !!\n",
    "        ## Find \"uv\", the new gaze location...\n",
    "        ## ADD GAUSSIAN NOISE TO THE MEMORY UPDATE !\n",
    "        self.new_pos = self.bp.name_variable(variable_name = \"pos_\" + self.name, \n",
    "                                        v = self.pos + tf.multiply(self.velocity,self.time_delta))\n",
    "        self.new_velocity = self.bp.name_variable(\"velocity_\" + self.name, \n",
    "                                        self.velocity + tf.multiply(tensor_velocity, self.time_delta))\n",
    "        self.new_gaze = self.bp.name_variable(\"gaze_\" + self.name, tensor_gaze)\n",
    "        self.new_memory = self.bp.name_variable(\"memory_\" + self.name, self.memory + tensor_memory_delta)\n",
    "        self.new_memory_last = self.bp.name_variable(\"memory_last_\" + self.name, self.memory_last + tensor_memory_last)\n",
    "        self.new_utterance = self.bp.name_variable(\"utterance_\" + self.name, tensor_utterance)\n",
    "\n",
    "                                            \n",
    "    def get_position(self):\n",
    "        return self.pos\n",
    "    \n",
    "    def get_velocity(self):\n",
    "        return self.velocity\n",
    "    \n",
    "    def get_gaze(self):\n",
    "        return self.gaze\n",
    "\n",
    "    def get_goal(self):\n",
    "        return self.goal\n",
    "    \n",
    "    def get_color(self):\n",
    "        return self.col\n",
    "    \n",
    "    def get_utterance(self):\n",
    "        return self.utterance\n",
    "                                        \n",
    "    def get_memory(self):\n",
    "        return self.memory\n",
    "    \n",
    "    def get_memory_last(self):\n",
    "        return self.memory_last\n",
    "    \n",
    "    def get_phys_state(self):\n",
    "        return (self.get_position(), self.get_velocity(), self.get_gaze(), self.get_col)\n",
    "    \n",
    "    def get_name_target(self):\n",
    "        return self.name_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    # Use this class to instantiate an environment on N batches. All batches share the same structure, but not not the\n",
    "    # same goals.\n",
    "    def __init__(self, nb_agents = 3, nb_landmarks = 0, env_dim = 2, batch_size = 1024, goal_size = 8):\n",
    "        self.env_dim = env_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.goal_size = goal_size\n",
    "        self.nb_agents = nb_agents\n",
    "        self.nb_landmarks = nb_landmarks\n",
    "        self.list_agents = []\n",
    "        self.list_phys_tensors = []\n",
    "        self.list_utter_tensors = []\n",
    "        self.list_mem_tensors = []\n",
    "        self.list_mem_last_tensors = []\n",
    "        self.list_goals_tensors = []\n",
    "        self.bp = BPTT()\n",
    "        \n",
    "        self.init_all()\n",
    "        \n",
    "    def init_agents(self):\n",
    "        for i in range(self.nb_agents):\n",
    "            name_of_targets = np.random.randint(0, self.nb_agents, (1, self.batch_size))\n",
    "            ag = Agent(name = str(i), name_target = name_of_targets, bp = self.bp, \n",
    "                       pos = np.transpose(np.array([[0, 0]])), v = np.transpose(np.array([[0, 0]])),\n",
    "                        gaze = np.transpose(np.array([[0, 0]])), col = (1, 2, 3))\n",
    "            self.list_agents.append(ag)\n",
    "            self.list_phys_tensors.append(ag.get_position())\n",
    "            self.list_utter_tensors.append(ag.get_utterance())\n",
    "            self.list_mem_tensors.append(ag.get_memory())\n",
    "            self.list_mem_last_tensors.append(ag.get_memory_last())\n",
    "            self.list_goals_tensors.append(ag.get_goal())\n",
    "     \n",
    "    def init_policy(self):\n",
    "        self.policy = Policy(self.nb_agents, self.nb_landmarks, self.list_phys_tensors, self.list_utter_tensors, \n",
    "                            self.list_mem_tensors, self.list_mem_last_tensors, self.list_goals_tensors, self.goal_size)\n",
    "        \n",
    "    def init_new_agents_states(self):\n",
    "        for i in range(self.nb_agents):\n",
    "            ag = self.list_agents[i]\n",
    "            tens_utterance = self.policy.list_utterance[i]\n",
    "            tens_velocity = self.policy.list_move[i]\n",
    "            tens_gaze = self.policy.list_gaze[i]\n",
    "            tens_mem_delta = self.policy.list_delta_mem_comm[i]\n",
    "            tens_mem_delta_last = self.policy.list_delta_mem_last[i]\n",
    "            ag.compute_new_state(tens_utterance, tens_velocity, tens_gaze, tens_mem_delta, tens_mem_delta_last)\n",
    "         \n",
    "\n",
    "    def init_reward_agents(self): ## Check the shuffle for the pos of agent and gaze is OK !!\n",
    "        rewards = []\n",
    "        ag_positions = []\n",
    "        ag_gazes = []\n",
    "        ag_goal_on_agent = []\n",
    "        for agent in self.list_agents:\n",
    "            ag_positions.append(agent.get_position())\n",
    "            ag_gazes.append(agent.get_gaze())\n",
    "            \n",
    "        agent_positions = tf.stack(ag_positions, axis = 2)\n",
    "        agent_gazes = tf.stack(ag_gazes, axis = 2)\n",
    "        \n",
    "        for i in range(self.nb_agents):\n",
    "            rewards = []\n",
    "            agent = self.list_agents[i]\n",
    "            name_target = agent.get_name_target()\n",
    "            position_target = tf.reshape(tf.slice(agent_positions, [0, 0, i], [self.env_dim, self.batch_size, 1]), \n",
    "                                         [self.env_dim, self.batch_size])\n",
    "            \n",
    "            gaze_target = tf.reshape(tf.slice(agent_gazes, [0, 0, i], [self.env_dim, self.batch_size, 1]), \n",
    "                                         [self.env_dim, self.batch_size])\n",
    "            \n",
    "            reward_agent = agent.compute_reward_agent(position_target, gaze_target)\n",
    "            rewards.append(reward_agent)\n",
    "            \n",
    "            self.rewards_batch = tf.reduce_sum(tf.concat(rewards, axis = 0), axis = 0)\n",
    "            \n",
    "            \n",
    "    def init_all(self):\n",
    "        self.init_agents()\n",
    "        self.init_policy()\n",
    "        self.init_new_agents_states()\n",
    "        self.init_reward_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-166-b6edec0ffd13>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-166-b6edec0ffd13>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class Experiment:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp = Environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = tf.constant([1, 2, 3])\n",
    "t2 = tf.reshape(t1, [1, 3])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    e1, e2 = sess.run([t1, t2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3]], dtype=int32)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
