{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from bptt import BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PhysicalNet: \n",
    "    \n",
    "    def __init__(self, layer_sizes, input_size, first_input, output_size, keep_prob = 0.9, stddev = 0.001):\n",
    "        self.nb_layers = len(layer_sizes)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.first_input = first_input\n",
    "        self.stddev = stddev\n",
    "        \n",
    "        self.Weights = []\n",
    "        self.Biases = []\n",
    "        self.intermediate_outputs = [self.first_input]\n",
    "        \n",
    "        self.init_weights()\n",
    "        self.init_biases()\n",
    "        self.define_inter_outputs()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for i in range(self.nb_layers):\n",
    "            if i == 0:\n",
    "                W = tf.Variable(tf.random_normal([self.layer_sizes[i], self.input_size], stddev = self.stddev))\n",
    "            elif i != (self.nb_layers - 1):\n",
    "                W = tf.Variable(tf.random_normal([self.layer_sizes[(i+1)], self.layer_sizes[i]], stddev = self.stddev))\n",
    "            else:\n",
    "                W = tf.Variable(tf.random_normal([self.output_size, self.layer_sizes[i]], stddev = self.stddev))\n",
    "                \n",
    "            self.Weights.append(W)\n",
    "            \n",
    "    def init_biases(self):\n",
    "        for i in range(self.nb_layers):\n",
    "            B = tf.Variable(tf.random_normal([self.layer_sizes[i], 1], stddev = self.stddev))\n",
    "            self.Biases.append(B)\n",
    "            \n",
    "    def define_inter_outputs(self): ## ADD THE DROPOUTS !!!\n",
    "        for i in range(self.nb_layers):\n",
    "            W = self.Weights[i]\n",
    "            b = self.Biases[i]\n",
    "            x = self.intermediate_outputs[i]\n",
    "            if i != (self.nb_layers - 1):\n",
    "                o = tf.nn.dropout(tf.nn.elu(tf.matmul(W, x) + b), keep_prob = 0.9)\n",
    "            else:\n",
    "                o = tf.nn.softmax(tf.matmul(W, x) + b)\n",
    "                \n",
    "            self.intermediate_outputs.append(o)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CommunicationNet: ## ADD THE MEMORY !! \n",
    "    \n",
    "    def __init__(self, layer_sizes, input_size, first_input, memory, keep_prob = 0.9, memory_size = 32,\n",
    "                 stddev_epsilon = 0.35, output_size = 256, stddev = 0.001):\n",
    "        self.nb_layers = len(layer_sizes)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.input_size = input_size\n",
    "        self.memory_size = memory_size\n",
    "        self.stddev_epsilon = stddev_epsilon\n",
    "        self.output_size = output_size\n",
    "        self.first_input = first_input\n",
    "        self.stddev = stddev\n",
    "        \n",
    "        self.Weights = []\n",
    "        self.Biases = []\n",
    "        self.Weight_read_mem = tf.Variable(tf.random_normal([self.layer_sizes[(self.nb_layers-1)], self.memory_size]\n",
    "                                                            ,stddev = self.stddev))\n",
    "        self.intermediate_outputs = [first_input]\n",
    "        self.memory = memory\n",
    "        \n",
    "        self.init_weights()    \n",
    "        self.init_biases()\n",
    "        self.define_inter_outputs()\n",
    "        self.def_delta_mem()\n",
    "        \n",
    "        \n",
    "    def init_weights(self):\n",
    "        for i in range(self.nb_layers):\n",
    "            if i == 0:\n",
    "                W = tf.Variable(tf.random_normal([self.layer_sizes[i], self.input_size], stddev = self.stddev))\n",
    "            elif i != (self.nb_layers - 1):\n",
    "                W = tf.Variable(tf.random_normal([self.layer_sizes[(i+1)], self.layer_sizes[i]],stddev = self.stddev))\n",
    "            else:\n",
    "                W = tf.Variable(tf.random_normal([self.output_size, self.layer_sizes[i]],stddev = self.stddev))\n",
    "            self.Weights.append(W)\n",
    "            \n",
    "    def init_biases(self):\n",
    "        for i in range(self.nb_layers):\n",
    "            B = tf.Variable(tf.random_normal([self.layer_sizes[i], 1],stddev = self.stddev))\n",
    "            self.Biases.append(B)\n",
    "            \n",
    "    def define_inter_outputs(self): ## ADD THE DROPOUTS !!!\n",
    "        for i in range(self.nb_layers):\n",
    "            W = self.Weights[i]\n",
    "            b = self.Biases[i]\n",
    "            x = self.intermediate_outputs[i]\n",
    "            if i != (self.nb_layers - 1):\n",
    "                o = tf.nn.dropout(tf.nn.elu(tf.matmul(W, x) + b), keep_prob = 0.9)\n",
    "            else:\n",
    "                o = tf.nn.softmax(tf.matmul(W, x) + tf.matmul(self.Weight_read_mem, self.memory) + b)\n",
    "                \n",
    "            self.intermediate_outputs.append(o)\n",
    "        \n",
    "    def def_delta_mem(self):\n",
    "        self.W_mem = tf.Variable(tf.random_normal(shape =[self.memory_size,self.output_size],stddev = self.stddev))\n",
    "        self.b_mem = tf.Variable(tf.random_normal(shape = [self.memory_size],stddev = self.stddev))\n",
    "        self.output_mem = tf.matmul(self.W_mem, self.intermediate_outputs[-1]) + self.b_mem\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LastNet: ## ADD THE MEMORY !! The memory initialization is random ==> set it 0\n",
    "    \n",
    "    def __init__(self, layer_sizes, input_size, first_input, memory, memory_delta, keep_prob = 0.9, memory_size = 32, \n",
    "                 stddev_epsilon = 0.35, output_size = 4, stddev = 0.001):\n",
    "        self.nb_layers = len(layer_sizes)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.input_size = input_size\n",
    "        self.memory_size = memory_size\n",
    "        self.stddev_epsilon = stddev_epsilon\n",
    "        self.output_size = output_size\n",
    "        self.first_input = first_input\n",
    "        self.stddev = stddev\n",
    "        \n",
    "        self.Weights = []\n",
    "        self.Biases = []\n",
    "        self.Weight_read_mem = tf.Variable(tf.random_normal([self.output_size, self.memory_size],stddev = self.stddev))\n",
    "        self.intermediate_outputs = [self.first_input]\n",
    "        self.memory = memory\n",
    "        self.memory_delta = memory_delta\n",
    "        \n",
    "        self.init_weights()\n",
    "            \n",
    "        self.init_biases()\n",
    "        self.def_write_memory()\n",
    "        self.define_inter_outputs()\n",
    "        self.def_delta_mem()\n",
    "        \n",
    "        \n",
    "    def init_weights(self):\n",
    "        for i in range(self.nb_layers):\n",
    "            if i == 0:\n",
    "                W = tf.Variable(tf.random_normal([self.layer_sizes[i], self.input_size],stddev = self.stddev))\n",
    "            elif i != (self.nb_layers - 1):\n",
    "                W = tf.Variable(tf.random_normal([self.layer_sizes[(i+1)], self.layer_sizes[i]],stddev = self.stddev))\n",
    "            else:\n",
    "                W = tf.Variable(tf.random_normal([self.output_size, self.layer_sizes[i]],stddev = self.stddev))\n",
    "            self.Weights.append(W)\n",
    "            \n",
    "    def init_biases(self):\n",
    "        for i in range(self.nb_layers):\n",
    "            if i != (self.nb_layers - 1):\n",
    "                B = tf.Variable(tf.random_normal([self.layer_sizes[i+1], 1],stddev = self.stddev))\n",
    "            else:\n",
    "                B = tf.Variable(tf.random_normal([self.output_size, 1], stddev = self.stddev))\n",
    "                \n",
    "            self.Biases.append(B)\n",
    "            \n",
    "    def define_inter_outputs(self): ## ADD THE DROPOUTS !!! REMOVE THE SOFTMAX OF THE LAST LAYER !!!\n",
    "        for i in range(self.nb_layers):\n",
    "            W = self.Weights[i]\n",
    "            b = self.Biases[i]\n",
    "            x = self.intermediate_outputs[i]\n",
    "            if i != (self.nb_layers - 1):\n",
    "                o = tf.nn.dropout(tf.nn.elu(tf.matmul(W, x) + b), keep_prob = 0.9)\n",
    "            else:\n",
    "                o = tf.matmul(W, x) + tf.matmul(self.Weight_read_mem, self.memory) + b\n",
    "                \n",
    "            self.intermediate_outputs.append(o)\n",
    "         \n",
    "    def def_write_memory(self):\n",
    "        self.memory = tf.tanh(self.memory + self.memory_delta + tf.random_normal([self.memory_size, 1], \n",
    "                                                                                 stddev = self.stddev_epsilon))\n",
    "        \n",
    "    def def_delta_mem(self):\n",
    "        self.W_mem = tf.Variable(tf.random_normal(shape =[self.memory_size,self.output_size],stddev = self.stddev))\n",
    "        self.b_mem = tf.Variable(tf.random_normal(shape = [self.memory_size],stddev = self.stddev))\n",
    "        self.output_mem = tf.matmul(self.W_mem, self.intermediate_outputs[-1]) + self.b_mem\n",
    "        \n",
    "    def get_output(self):\n",
    "        return self.intermediate_outputs[-1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    \n",
    "    def __init__(self, nb_agent, nb_landmark, vocabulary_size, hidden_layer_size = 256, env_dim = 2, \n",
    "                 size_goal = 8, memory_size = 32, nb_actions = 3, temperature = 1, batch_size = 1024,\n",
    "                stddev_phys_output = 0.01):\n",
    "        self.stddev_phys_output = stddev_phys_output\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        self.nb_actions = nb_actions\n",
    "        self.memory_size = memory_size\n",
    "        self.agent_name = agent_name\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.env_dim = env_dim\n",
    "        self.nb_agent = nb_agent\n",
    "        self.nb_landmark = nb_landmark\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.size_goal = size_goal\n",
    "        self.goal = tf.placeholder(tf.float32, [self.size_goal, None])\n",
    "        self.memory_last = tf.placeholder(tf.float32, [self.memory_size, None])\n",
    "        self.memorydelta_last = tf.placeholder(tf.float32, [self.memory_size, None])\n",
    "        \n",
    "        self.placeholders_com = []\n",
    "        self.placeholders_phys = []\n",
    "        self.placeholders_mem_comm = []\n",
    "        self.placeholders_deltamem_comm = []\n",
    "        self.networks_com = []\n",
    "        self.networks_phys = []\n",
    "        self.Phi = None\n",
    "        \n",
    "        self.PhiX = None\n",
    "        self.PhiC = None\n",
    "        self.utterances = None\n",
    "        \n",
    "        self.init_all()\n",
    "        \n",
    "    def init_placeholders_com(self):\n",
    "        for i in range(self.nb_agent):\n",
    "            self.placeholders_com.append(tf.placeholder(tf.float32, [self.vocabulary_size, None]))\n",
    "\n",
    "    def init_placeholders_phys(self):\n",
    "        for i in range((self.nb_agent + self.nb_landmark)):\n",
    "            self.placeholders_phys.append(tf.placeholder(tf.float32, [self.env_dim, None]))\n",
    "            \n",
    "    def init_placeholders_mem_comm(self):\n",
    "        for i in range(self.nb_agent):\n",
    "            self.placeholders_mem_comm.append(tf.placeholder(tf.float32, [self.memory_size, None]))\n",
    "    \n",
    "    def init_placeholders_deltamem_comm(self):\n",
    "        for i in range(self.nb_agent):\n",
    "            self.placeholders_deltamem_comm.append(tf.placeholder(tf.float32, [self.memory_size, None]))\n",
    "\n",
    "    def init_com_modules(self):## Les poids seront les mêmes pour tous les agents\n",
    "        with tf.variable_scope(\"communication\") as scope:\n",
    "            self.networks_com.append(CommunicationNet([self.hidden_layer_size, self.hidden_layer_size], \n",
    "                                                 self.vocabulary_size, self.placeholders_com[0], \n",
    "                                                      self.placeholders_mem_comm[0],\n",
    "                                                     self.placeholders_deltamem_comm[0]))\n",
    "        for i in range(1, self.nb_agent):\n",
    "            with tf.variable_scope(\"communication\", reuse=True):\n",
    "                self.networks_com.append(CommunicationNet([self.hidden_layer_size, self.hidden_layer_size], \n",
    "                                                 self.vocabulary_size,self.placeholders_com[i],\n",
    "                                                          self.placeholders_mem_comm[i],\n",
    "                                                         self.placeholders_deltamem_comm[i]))\n",
    "\n",
    "    def init_phys_modules(self):## Les poids seront les mêmes pour tous les agents, il faut rajouter un nom dans le scope\n",
    "        with tf.variable_scope(\"physical\") as scope:\n",
    "            self.networks_phys.append(PhysicalNet([self.hidden_layer_size, self.hidden_layer_size], self.env_dim, \n",
    "                                                  self.placeholders_phys[0],\n",
    "                                                 self.hidden_layer_size))\n",
    "        for i in range(1, (self.nb_agent + self.nb_landmark)):\n",
    "            with tf.variable_scope(\"physical\", reuse = True):\n",
    "                self.networks_phys.append(PhysicalNet([self.hidden_layer_size, self.hidden_layer_size], self.env_dim,\n",
    "                                                      self.placeholders_phys[i],\n",
    "                                                     self.hidden_layer_size))\n",
    "\n",
    "    def init_PhiX(self):\n",
    "        list_outputs = []\n",
    "        for net in self.networks_phys:\n",
    "            list_outputs.append(tf.reshape(net.intermediate_outputs[-1], [256, -1, 1]))\n",
    "\n",
    "        all_phys_output = tf.concat(list_outputs, axis = 2)\n",
    "        self.PhiX = tf.reduce_max(tf.nn.softmax(all_phys_output, dim = -1), axis = 2)\n",
    "\n",
    "    def init_PhiC(self):\n",
    "        list_outputs = []\n",
    "        for net in self.networks_com:\n",
    "            list_outputs.append(tf.reshape(net.intermediate_outputs[-1], [256, -1, 1]))\n",
    "\n",
    "        all_comm_output = tf.concat(list_outputs, axis = 2)\n",
    "        self.PhiC = tf.reduce_max(tf.nn.softmax(all_comm_output, dim = -1), axis = 2) \n",
    "\n",
    "    def init_Phi(self):\n",
    "        self.Phi = tf.concat([self.PhiC, self.goal, self.PhiX], axis = 0)\n",
    "\n",
    "    def init_last_module(self):\n",
    "        inp_size = (2*self.hidden_layer_size + self.size_goal)\n",
    "        self.last_net = LastNet([256, 256], inp_size, self.Phi\n",
    "                                , self.memory_last, self.memorydelta_last)\n",
    "        \n",
    "    def create_feed_dict(self, list_positions, list_utterances, list_mem_comm, list_deltamem_comm, list_mem_last,\n",
    "                  list_detlamem_last, goal):\n",
    "        feed_dict_com = {tensor:com for tensor,com in zip(self.placeholders_com, list_utterances)}\n",
    "        feed_dict_phys = {tensor:phys for tensor,phys in zip(self.placeholders_phys, list_positions)}\n",
    "        feed_dict_mem_com = {tensor:mem for tensor,mem in zip(self.placeholders_mem_comm, list_mem_comm)}\n",
    "        feed_dict_deltamem_com = {tensor:mem for tensor,mem in zip(self.placeholders_deltamem_comm, list_deltamem_comm)}\n",
    "        feed_dict_last = {self.memory_last:list_mem_last[0], self.memorydelta_last:list_deltamem_comm[0]}\n",
    "        feed_dict_goal = {self.goal: goal[0]}\n",
    "        feed_dict_all = {}\n",
    "        feed_dict_all.update(feed_dict_com)\n",
    "        feed_dict_all.update(feed_dict_phys)\n",
    "        feed_dict_all.update(feed_dict_mem_com)\n",
    "        feed_dict_all.update(feed_dict_deltamem_com)\n",
    "        feed_dict_all.update(feed_dict_last)\n",
    "        feed_dict_all.update(feed_dict_goal)\n",
    "        return feed_dict_all\n",
    "        \n",
    "    def init_sample_utterances(self):## Vérifier qu'on prend un bon slice sur l'output\n",
    "        u = -tf.log(-tf.log(tf.random_uniform(shape = [self.vocabulary_size, self.batch_size],dtype=tf.float32)))\n",
    "        utterance_output = tf.slice(self.output, [self.env_dim, 0], [self.vocabulary_size, self.batch_size])\n",
    "        gumbel = tf.exp((utterance_output + u)/self.temperature)\n",
    "        denoms = tf.reduce_sum(gumbel, axis = 0)\n",
    "        self.utterance = gumbel/denoms  \n",
    "        \n",
    "    def init_sample_phys(self):\n",
    "        u = tf.random_normal(shape = [self.env_dim, self.batch_size],dtype=tf.float32, stddev = self.stddev_phys_output)\n",
    "        phys_output = tf.slice(self.output, [0, 0], [self.env_dim, self.batch_size])\n",
    "        self.sample_move = phys_output + u\n",
    "        \n",
    "    def init_output(self):\n",
    "        self.output = self.last_net.get_output()\n",
    "\n",
    "    def init_all(self):\n",
    "        self.init_placeholders_com()\n",
    "        self.init_placeholders_phys()\n",
    "        self.init_placeholders_mem_comm()\n",
    "        self.init_placeholders_deltamem_comm()\n",
    "        self.init_com_modules()\n",
    "        self.init_phys_modules()\n",
    "        self.init_PhiX()\n",
    "        self.init_PhiC()\n",
    "        self.init_Phi()\n",
    "        self.init_last_module()\n",
    "        self.init_output()\n",
    "        self.init_sample_utterances()\n",
    "        self.init_sample_phys()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy_Phys:\n",
    "    \n",
    "    def __init__(self, nb_agent, nb_landmark, list_phys_tensors, hidden_layer_size = 256, env_dim = 2, \n",
    "                 batch_size = 1024, stddev_phys_output = 0.01):\n",
    "        self.stddev_phys_output = stddev_phys_output\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_actions = nb_actions\n",
    "        self.env_dim = env_dim\n",
    "        self.nb_agent = nb_agent\n",
    "        self.nb_landmark = nb_landmark\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.phys_tensors = phys_tensors\n",
    "        self.networks_phys = []\n",
    "        self.PhiX = None\n",
    "        \n",
    "        self.init_all()\n",
    "        \n",
    "    def init_phys_modules(self):\n",
    "        with tf.variable_scope(\"physical\") as scope:\n",
    "            self.networks_phys.append(PhysicalNet([self.hidden_layer_size, self.hidden_layer_size], self.env_dim, \n",
    "                                                  self.phys_tensors[0],\n",
    "                                                 self.hidden_layer_size))\n",
    "        for i in range(1, (self.nb_agent + self.nb_landmark)):\n",
    "            with tf.variable_scope(\"physical\", reuse = True):\n",
    "                self.networks_phys.append(PhysicalNet([self.hidden_layer_size, self.hidden_layer_size], self.env_dim,\n",
    "                                                      self.phys_tensors[i],\n",
    "                                                     self.hidden_layer_size))\n",
    "\n",
    "    def init_PhiX(self):\n",
    "        list_outputs = []\n",
    "        for net in self.networks_phys:\n",
    "            list_outputs.append(tf.reshape(net.intermediate_outputs[-1], [256, -1, 1]))\n",
    "\n",
    "        all_phys_output = tf.concat(list_outputs, axis = 2)\n",
    "        self.PhiX = tf.reduce_max(tf.nn.softmax(all_phys_output, dim = -1), axis = 2)\n",
    "\n",
    "    def init_all(self):\n",
    "        self.init_phys_modules()\n",
    "        self.init_PhiX()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy_Utterance:\n",
    "    \n",
    "    def __init__(self, list_utter_tensors, list_mem_tensors, vocabulary_size = 20, \n",
    "                 hidden_layer_size = 256, memory_size = 32, temperature = 1, batch_size = 1024,\n",
    "                 stddev_phys_output = 0.01):\n",
    "        self.stddev_phys_output = stddev_phys_output\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        self.memory_size = memory_size\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.goal = tf.placeholder(tf.float32, [self.size_goal, None])\n",
    "        self.memory_last = tf.placeholder(tf.float32, [self.memory_size, None])\n",
    "        \n",
    "        self.com_tensors = list_utter_tensors\n",
    "        self.mem_tensors = list_mem_tensors\n",
    "        self.delta_mem = []\n",
    "        self.networks_com = []\n",
    "\n",
    "        self.PhiC = None\n",
    "        self.init_all()\n",
    "        \n",
    "\n",
    "    def init_com_modules(self):## Les poids seront les mêmes pour tous les agents\n",
    "        with tf.variable_scope(\"communication\") as scope:\n",
    "            self.networks_com.append(CommunicationNet([self.hidden_layer_size, self.hidden_layer_size], \n",
    "                                                 self.vocabulary_size, self.com_tensors[0], \n",
    "                                                      self.mem_tensors[0]))\n",
    "        for i in range(1, self.nb_agent):\n",
    "            with tf.variable_scope(\"communication\", reuse=True):\n",
    "                self.networks_com.append(CommunicationNet([self.hidden_layer_size, self.hidden_layer_size], \n",
    "                                                 self.vocabulary_size,self.com_tensors[i],\n",
    "                                                          self.mem_tensors[i]))\n",
    "\n",
    "    def init_delta_mem_tensors(self):\n",
    "        for i in range(self.nb_agents):\n",
    "            self.delta_mem.append(self.networks_com[i].output_mem)\n",
    "            \n",
    "    def init_PhiC(self):\n",
    "        list_outputs = []\n",
    "        for net in self.networks_com:\n",
    "            list_outputs.append(tf.reshape(net.intermediate_outputs[-1], [256, -1, 1]))\n",
    "\n",
    "        all_comm_output = tf.concat(list_outputs, axis = 2)\n",
    "        self.PhiC = tf.reduce_max(tf.nn.softmax(all_comm_output, dim = -1), axis = 2) \n",
    "\n",
    "    def init_all(self):\n",
    "        self.init_com_modules()\n",
    "        self.init_delta_mem_tensors\n",
    "        self.init_PhiC()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy_Last:\n",
    "    \n",
    "    def __init__(self, PhiX, PhiC, goal, memory, hidden_layer_size = 256, \n",
    "                 size_goal = 8, memory_size = 32, batch_size = 1024, stddev_phys_output = 0.01, vocabulary_size = 20,\n",
    "                nv_dim = 2):\n",
    "        self.stddev_phys_output = stddev_phys_output\n",
    "        self.env_dim = env_dim\n",
    "        self.vocabulary_size = vocabulary_size \n",
    "        self.batch_size = batch_size\n",
    "        self.memory_size = memory_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.size_goal = size_goal\n",
    "        self.goal = goal\n",
    "        self.memory_last = memory\n",
    "        self.Phi = None \n",
    "        self.PhiX = PhiX\n",
    "        self.PhiC = PhiC \n",
    "        self.delta_mem = []\n",
    "        self.init_all()\n",
    "\n",
    "    def init_Phi(self):\n",
    "        self.Phi = tf.concat([self.PhiC, self.goal, self.PhiX], axis = 0)\n",
    "\n",
    "    def init_last_module(self):\n",
    "        inp_size = (2*self.hidden_layer_size + self.size_goal)\n",
    "        self.last_net = LastNet([256, 256], inp_size, self.Phi, self.memory_last)\n",
    "        \n",
    "    def init_output(self):\n",
    "        self.output = self.last_net.get_output()\n",
    "\n",
    "    def init_all(self):\n",
    "        self.init_Phi()\n",
    "        self.init_last_module()\n",
    "        self.init_output()\n",
    "        self.init_sample_utterances()\n",
    "        self.init_sample_phys()\n",
    "\n",
    "\n",
    "    def init_sample_utterances(self):## Vérifier qu'on prend un bon slice sur l'output\n",
    "        u = -tf.log(-tf.log(tf.random_uniform(shape = [self.vocabulary_size, self.batch_size],dtype=tf.float32)))\n",
    "        utterance_output = tf.slice(self.output, [self.env_dim, 0], [self.vocabulary_size, self.batch_size])\n",
    "        gumbel = tf.exp((utterance_output + u)/self.temperature)\n",
    "        denoms = tf.reduce_sum(gumbel, axis = 0)\n",
    "        self.utterance = gumbel/denoms  \n",
    "        \n",
    "    def init_sample_phys(self):\n",
    "        u = tf.random_normal(shape = [self.env_dim, self.batch_size],dtype=tf.float32, stddev = self.stddev_phys_output)\n",
    "        phys_output = tf.slice(self.output, [0, 0], [self.env_dim, self.batch_size])\n",
    "        self.sample_move = phys_output + u\n",
    "        \n",
    "    def init_delta_mem_tensors(self):\n",
    "        for i in range(self.nb_agents):\n",
    "            self.delta_mem.append(self.networks_com[i].output_mem)\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy:# Two memories per Agent: one for the communication module, the other one for the last module. Is it correct ?\n",
    "\n",
    "    def __init__(self,nb_agent, nb_landmark, list_phys_tensors, list_utter_tensors, list_mem_tensors, list_goal_tensors,\n",
    "                 vocabulary_size = 20, hidden_layer_size = 256, memory_size = 32, temperature = 1, batch_size = 1024,\n",
    "                 stddev_phys_output = 0.01, env_dim = 2):\n",
    "        self.nb_agent = nb_agent\n",
    "        self.nb_landmark = nb_landmark\n",
    "        self.list_phys_tensors =  list_phys_tensors\n",
    "        self.list_utter_tensors = list_utter_tensors\n",
    "        self.list_mem_tensors = list_mem_tensors\n",
    "        self.list_goal_tensors = list_goal_tensors\n",
    "        \n",
    "        self.phys_module = Policy_Phys(self.nb_agent, self.nb_landmark, self.list_phys_tensors)\n",
    "        self.utterance_module = Policy_Utterance(self.list_utter_tensors, self.list_mem_tensors)\n",
    "        \n",
    "        self.list_last_nets = []\n",
    "        self.list_delta_mem_comm = self.utterance_module.delta_mem\n",
    "        self.list_delta_mem_last = []\n",
    "        self.list_outputs = []\n",
    "        \n",
    "    def init_last_nets(self):\n",
    "        for i in range(self.nb_agent):\n",
    "            self.list_last_nets.append(Policy_Last(self.phys_module.PhiX, self.utterance_module.PhiC, \n",
    "                                                   self.list_goal_tensors[i], self.list_mem_tensors[i]))\n",
    "            \n",
    "            \n",
    "    def init_output_list(self):\n",
    "        for i in range(self.nb_agent):\n",
    "            self.list_utterance.append(self.list_last_nets[i].utterance)\n",
    "            self.list_move.append(self.list_last_nets[i].sample_move)\n",
    "            self.list_delta_mem_last.append(self.list_last_nets[i].delta_mem)\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, name, bp, pos, v, gaze, col, vocabulary_size = 20, batch_size = 1024, goals = [0, 0, 1], \n",
    "                 env_dim = 2, goal_size = 8, memory_size = 32, time_delta = 0.1):\n",
    "        self.time_delta = tf.constant([time_delta])\n",
    "        self.env_dim = env_dim\n",
    "        self.memory_size = memory_size\n",
    "        self.name = name\n",
    "        self.goal_size = goal_size\n",
    "        self.batch_size = batch_size\n",
    "        self.bp = bp\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        \n",
    "        self.pos = self.bp.get_past_variable(variable_name = \"pos_\" + self.name, starting_value = \n",
    "                                        tf.placeholder(tf.float32, shape = [self.env_dim, None])) \n",
    "        self.velocity = self.bp.get_past_variable(\"velocity_\" + self.name,tf.placeholder(tf.float32, \n",
    "                                                                                    shape = [self.env_dim, None])) \n",
    "        self.gaze = self.bp.get_past_variable(\"gaze_\" + self.name, tf.placeholder(tf.float32, \n",
    "                                                                             shape = [self.env_dim, None]))\n",
    "        self.goal = tf.placeholder(tf.float32, shape = [(self.goal_size + self.env_dim), None])\n",
    "        self.utterance = self.bp.get_past_variable(\"utterance_\" + self.name, \n",
    "                                              tf.placeholder(tf.float32, shape = [self.vocabulary_size, None]))\n",
    "        \n",
    "        self.memory = self.bp.get_past_variable(\"memory_\" + self.name, tf.zeros(shape = [self.memory_size, \n",
    "                                                                                         self.batch_size]))\n",
    "        self.memory_delta = self.bp.get_past_variable(\"memory_delta_\" + self.name, tf.zeros(shape = [self.memory_size, \n",
    "                                                                                                     self.batch_size]))\n",
    "        self.memory_last = self.bp.get_past_variable(\"memory_last_\" + self.name, tf.zeros(shape = [self.memory_size, \n",
    "                                                                                                   self.batch_size]))\n",
    "        self.col = col\n",
    "        \n",
    "        #self.policy = Policy(self.nb_agent, self.nb_landmark, self.vocabulary_size)\n",
    "        \n",
    "    def take_step(self, list_positions, list_utterances, list_mem_comm, list_deltamem_comm, list_mem_last,\n",
    "              list_detlamem_last, goal, session):\n",
    "        feed_dict_all = self.create_feed_dict(list_positions, list_utterances, list_mem_comm, list_deltamem_comm, \n",
    "                list_mem_last, list_detlamem_last, goal)\n",
    "        return session.run([self.utterances, self.sample_move], feed_dict = feed_dict_all) \n",
    "\n",
    "    def get_move(self):\n",
    "        return self.p.sample_move\n",
    "    \n",
    "    def get_utterance(self):\n",
    "        return self.p.utterance\n",
    "    \n",
    "    def init_reward(self):\n",
    "        goal_on_agent = tf.slice(self.goal, [0, 0], [self.env_dim, self.batch_size])\n",
    "        goal_type = tf.slice(self.goal, [self.env_dim, 0], [self.goal_size, self.batch_size])\n",
    "        r1 = tf.square(tf.norm(tensor_goal_pos - goal_on_agent, axis = 0))\n",
    "        r2 = tf.square(tf.norm(tensor_goal_look - goal_on_agent, axis = 0))\n",
    "    \n",
    "    def compute_reward_agent(self,tensor_goal, tensor_goal_pos, tensor_goal_look, tensor_move, tensor_utterance):\n",
    "        ### Il faut prendre la position pour le goal et non pas l'agent spécifique.\n",
    "        goal_on_agent = tf.slice(tensor_goal, [0, 0], [self.env_dim, self.batch_size])\n",
    "        goal_type = tf.slice(tensor_goal, [self.env_dim, 0], [self.goal_size, self.batch_size])\n",
    "        r1 = tf.square(tf.norm(tensor_goal_pos - goal_on_agent, axis = 0))\n",
    "        r2 = tf.square(tf.norm(tensor_goal_look - goal_on_agent, axis = 0))\n",
    "        utt_norm = tf.square(tf.norm(tensor_utterance, axis = 0))\n",
    "        move_norm = tf.square(tf.norm(tensor_move, axis = 0))\n",
    "        vec = tf.concat([r1, r2, tf.zeros([1,self.batch_size]), tf.float32], axis = 0)\n",
    "        v1 = tf.matmult(vec, tensor_type)\n",
    "        r = -(v1 + utt_norm + move_norm)\n",
    "        return r\n",
    "            \n",
    "    def compute_new_phys_state(self, tensor_velocity, tensor_gaze):## ADD THE FORCES TO THE NEW VELOCITY !!\n",
    "        ## Find \"uv\", the new gaze location...\n",
    "        new_pos = self.pos + tf.mul(self.velocity*self.time_delta)\n",
    "        new_velocity = self.velocity + tf.mul(tensor_utterance, self.time_delta)\n",
    "        new_gaze = tensor_gaze\n",
    "        \n",
    "        return new_pos, new_velocity, new_gaze\n",
    "    \n",
    "    \n",
    "    def set_new_phys_state(self, new_pos, new_velocity, new_gaze):\n",
    "        \n",
    "    \n",
    "    def get_position(self):\n",
    "        return self.pos\n",
    "    \n",
    "    def get_velocity(self):\n",
    "        return self.velocity\n",
    "    \n",
    "    def get_gaze(self):\n",
    "        return self.gaze\n",
    "\n",
    "    def get_goals(self):\n",
    "        return self.goal\n",
    "    \n",
    "    def get_color(self):\n",
    "        return self.col\n",
    "    \n",
    "    def get_phys_state(self):\n",
    "        return (self.get_position(), self.get_velocity(), self.get_gaze(), self.get_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Exp:\n",
    "    \n",
    "    def __init__(self, nb_agents = 3, env_dim = 2, batch_size = 1024, goal_size = 3):\n",
    "        self.env_dim = env_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.goal_size = goal_size\n",
    "        self.nb_agents = nb_agents\n",
    "        self.list_agents = []\n",
    "        self.bp = BPTT()\n",
    "        \n",
    "        self.init_agents()\n",
    "        \n",
    "        \n",
    "    def init_agents(self):\n",
    "        for i in range(self.nb_agents):\n",
    "            self.list_agents.append(Agent(name = \"AA\", bp = self.bp, pos = np.transpose(np.array([[0, 0]])), v = np.transpose(np.array([[0, 0]])),\n",
    "                                         gaze = np.transpose(np.array([[0, 0]])), col = (1, 2, 3)))\n",
    "        \n",
    "    def compute_reward_agent(self,tensor_goal, tensor_goal_pos, tensor_goal_look, tensor_move, tensor_utterance):\n",
    "        ### Il faut prendre la position pour le goal et non pas l'agent spécifique.\n",
    "        goal_on_agent = tf.slice(tensor_goal, [0, 0], [self.env_dim, self.batch_size])\n",
    "        goal_type = tf.slice(tensor_goal, [self.env_dim, 0], [self.goal_size, self.batch_size])\n",
    "        r1 = tf.square(tf.norm(tensor_goal_pos - goal_on_agent, axis = 0))\n",
    "        r2 = tf.square(tf.norm(tensor_goal_look - goal_on_agent, axis = 0))\n",
    "        utt_norm = tf.square(tf.norm(tensor_utterance, axis = 0))\n",
    "        move_norm = tf.square(tf.norm(tensor_move, axis = 0))\n",
    "        vec = tf.concat([r1, r2, tf.zeros([1,self.batch_size]), tf.float32], axis = 0)\n",
    "        v1 = tf.matmult(vec, tensor_type)\n",
    "        r = -(v1 + utt_norm + move_norm)\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp = Exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = Policy(nb_agent = 1,nb_landmark = 0, vocabulary_size = 2, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [np.array([[0.001, 0.001]]).transpose()]\n",
    "c = [np.array([[0.001, 0.001]]).transpose()]\n",
    "mem_comm = [np.array([[0.001 for i in range(32)]]).transpose()]\n",
    "deltamem_comm = [np.array([[0.001 for i in range(32)]]).transpose()]\n",
    "mem_last = [np.array([[0.001 for i in range(32)]]).transpose()]\n",
    "deltamem_last = [np.array([[0.001 for i in range(32)]]).transpose()]\n",
    "goal = [np.array([[0.001 for i in range(8)]]).transpose()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    out = p.take_step(x, c, mem_comm, deltamem_comm, mem_last, deltamem_last, goal, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.43180865],\n",
       "       [ 0.56819129]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat_4:0' shape=(520, ?) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.last_net.first_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bp.BPTT.get"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
